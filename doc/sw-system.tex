%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% Space Warps I: Experiment Design
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\documentclass[useAMS,usenatbib,a4paper]{mn2e}
%% letterpaper
%% a4paper

\voffset=-0.6in

% Packages:
\input psfig.sty
\usepackage{xspace}
\usepackage{graphicx}
\usepackage{amssymb}
\usepackage{amsmath}

% Macros:
\input{macros.tex}
\input{addresses.tex}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\title[\sw I]
{\SW: I. Crowd-sourcing the Discovery of Gravitational Lenses}
    
\author[Marshall et al.]{%
  \input{sw-system-authors.tex}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}
             
\date{to be submitted to MNRAS}
\pagerange{\pageref{firstpage}--\pageref{lastpage}}\pubyear{2013}

\maketitle           

\label{firstpage}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{abstract} 

\sw is a web-based service
 that enables the discovery of strong
gravitational lenses in wide-field imaging surveys by arbitrarily large
numbers of people. Carefully produced color composite images are
displayed to volunteers via a flexible interface, which records
their estimates of the positions of candidate lensed features. Simulated
lenses, and expert-classified non-lenses, are inserted into the stream
at random intervals; this training set is used to give the volunteers
feedback on their performance as well as estimate a dynamically-updated
probability for any given image containing a lens. High probability
systems are filtered into the Zooniverse \Talk discussion system; this
smaller sample is further classified and analysed by volunteers into a
final set. We analyze the classification of the training set itself, and
find that 1) $<$volunteers learn$>$... 2) $<$the filter works$>$... 

\end{abstract}

% Full list of options at http://www.journals.uchicago.edu/ApJ/instruct.key.html

\begin{keywords}
  gravitational lensing   --
  methods: statistical    
\end{keywords}

\setcounter{footnote}{1}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Introduction}
\label{sec:intro}

Scientific motivation. Applications of lenses: group-scale arcs,
galaxy-galaxy lenses, lensed quasars. 

Problem of rarity. Imaging surveys. Problem of purity/false positives.

Review of progress to date. Methods in SL2S, SQLS. Contrast with SLACS. 

Scaling to wide field era. Automated methods: problems. Need for good
training sets. Need for quality control: always present.

Novel solution: crowd-sourcing. Brief review of similar problems.
PlanetHunters. \sw as an experiment.

In this paper, we describe the \sw website, an online system that
enables crowd-sourced detection of gravitational lenses. 
Other papers in this series will present new gravitational lenses
discovered in our first imaging survey dataset, and investigate the
differences between lens detections made in \sw and those made with
automated techniques. Here, we try to answer the following questons:
\begin{itemize}

\item How reliably can we find gravitational lenses using the \sw
system? What is the completeness of the sample produced?

\item How noisy is the system? What is the purity of the sample
produced?

\item How accurately can gravitational lenses be located on the sky
during the identification process?

\item How quickly can lenses be detected, and non-lenses be rejected?
How many volunteers are needed per target, and how quickly can they
mobilise?

\end{itemize}

In \Sref{sec:design} we introduce the \sw system, describing and
explaining its various features. We then briefly 



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Experiment Design}
\label{sec:design}

Unfamiliar objects: need to learn what lenses look like, fast. Rare
objects: need to be able to reject rapidly, and get through sample.
Confusion with non-lenses: further filtering via scientific discussion
and further classification (voting). 

% % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % 

\subsection{Tutorial Material}
\label{sec:design:tutorial}

Learning what lenses do: Spotter's Guide and LensToy. Learning how
lenses work (science page, FAQ). 

Inline tutorial. Merge into stream. Instant feedback, positive and
negative. Anecdotal support for this.


% % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % 

\subsection{The Classification Interface}
\label{sec:design:classification}

Image presentation. Consistency in size and appearance. 

Interface fast due to pre-loading of images, and minimizing interaction.

Spotting lenses: Markers to be placed. Two reasons: first, to give good
feedback. Second, to improve quality of discussion later.

Quick dashboard provides simple ways to explore further: zoom, contrast
controls.

Non-lenses marked? Favourite button instead, enabling serendipitous
discovery of other interesting things, separate from lenses.


% % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % 

\subsection{Collaborative Filtering with ``Talk''}
\label{sec:design:classification}

Feeding in collections, following online analysis. 
Taking subjects to Talk individually, too.

Mental modeling. Further inspection on Dashboard. Voting. 

Final sample generation.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Data}
\label{sec:data}

Training data and test data. Refer to Paper II for CFHTLS details.

Presentation of images. Arcsinh stretch, calibration. Approximately
optimized.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Identification Analysis}
\label{sec:IDanalysis}

In this section we outline our methodology for interpreting the
interactions of the volunteers with the identification interface.  Each
classification made is logged in a database, storing subject IDs,
(anonymous) volunteer IDs, a timestamp and the classification results. 
The {\it kind} of subject -- whether it is a training subject (a 
simulated lens or a known non-lens) or a test subject (an unseen image
drawn from the survey) -- is also recorded. For all subjects, the
positions of all Markers are recorded, in pixel coordinates. For
training subjects, we also store the ``classification'' of the subject
as a lens, or a non-lens, and also the type of object present in the
image. These types are summarized in \Tref{tab:objecttypes}. 
This classification is used to provide instant feedback, but is also the
basic measurement used in a probabilistic classification of every
subject based on all image views to date.

We perform a ``Pseudo-online'' analysis of the classifications, 
updating a probabilistic model of every (anonymous) volunteer's
data, and also updating the lens probability of each subject 
(in both the training and test sets), on a
daily(??) basis. This allows us to track the speed with which the crowd
learns about lenses, and also gives us a dynamic estimate of the
posterior probability for  any given  subject being a lens, given all
classifications of it. Assigning thresholds in this lens probability
allows us to make good decisions about whether or not to accept a
subject into the collection of candidates visible in \Talk, and also
whether or not to  carry on classifying a subject at all. 

In this paper we focus on the training data,  investigating  how the 
crowd's ability to identify gravitational lenses during the course of
the project, and the completeness and purity of the lens candidate
sample generated.

Where to describe probabilistic classifier stuff? Here or in an
appendix? Not sure.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Identification Results}
\label{sec:IDresults}

Understanding crowd, so we can help them learn faster. Understanding
images given the crowd, so we can find lenses.

% % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % 

\subsection{Learned Behavior}
\label{sec:results:learning}

% % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % 

\subsection{Sample completeness and purity}
\label{sec:results:learning}

Vary the acceptance and rejection thresholds, look at completeness and
purity as a function of this. How much time can be saved by retiring
subjects into candidate list or rejection list?


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Discussion}
\label{sec:discuss}

Challenges for future.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Conclusions}
\label{sec:conclude}

We draw the following conclusions:

\begin{itemize} 

\item Ability to detect lenses

\item What we can say about the next steps

\end{itemize}

Summarize, end.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%  ACKNOWLEDGMENTS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section*{Acknowledgements}
 
\input{acknowledgments.tex}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%  APPENDICES
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\appendix

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Probabilistic Classification Data Model}
\label{appendix:probmodel}

Our aim is to enable the construction of a sample of good lens candidates.
Since we aspire to making logical  decisions, we define a  ``good candidate''
as one which has a high posterior probability of being a lens, given the data:
$\pr(\LENS|\data)$. Our problem is to approximate this probability. The data
in our case are the pixel values of a colour image. However, we can greatly
compress these complex, noisy sets of data by asking each volunteer what they
think about them. A complete  classification in \sw consists of a set of
Marker positions, or none at all. The null set encodes the statement from
the volunteer that the image in question is $\saidNOT$ a lens, while the
placement of any  Markers indicates that the volunteer considers this image to
contain a $\saidLENS$.  We simplify the problem by only using the Marker
positions to assess whether t
he volunteer  correctly assigned the
classification $\saidLENS$ or $\saidNOT$ after viewing (blindly) a member of
the training set of subjects. 

How should we model these compressed data? The circumstances of each
classification are quite complex: the volunteers learn more about the problem
as they go, but also inevitably make occasional mistakes (perhaps because a
lens is difficult to see, or they became distracted by the television). To
cope with this uncertainty, we assign a software agent to partner each
volunteer. The agent's task is to interpret their volunteer's classification
data as best it can, using a model that makes a number of necessary
approximations. These interpretations will then include uncertainty arising as
a result of the volunteer's efforts and also the agent's approximations, but
they will have two important redeeming features. First, the interpretations
will be quantitative (where before they were qualititative),  and thus will be
useful in decision-making. Second, the agent will be able to predict, using
its model, the probability of a test subject being a $\LENS$, given both its
volunteer's classification, and its volunteer's experience. We now
describe how each agent works.

Each agent assumes that the probability of a volunteer recognising any given
simulated lens as a lens is some number, $\pr(\saidLENS|\LENS,T)$, that
depends only on what the volunteer is currently looking at, and all the
previous training subjects they have seen (and not on what type of lens it is,
how faint it is, what time it is, \etc). Likewise, it also assumes that the
probability of a volunteer recognising any given dud image as a dud is some
other number, $\pr(\saidNOT|\NOT,T)$, that also depends only on what the volunteer is currently looking at, and all the
previous training subjects they have seen. These two probabilities define a 
2 by 2 ``confusion matrix,'' which the agent updates, every time a
volunteer classifies a training subject, using the following 
very simple estimate:
\be
  \pr(``X"|X,T) \approx \frac{N_{``X"}}{N_X}.
  \label{eq:app:fraction}
\ee
Here, $X$ stands for the true classification of the subject, \ie either
$\LENS$ or $\NOT$, while $``X''$ is the corresponding classification
made by the volunteer on viewing the subject. $N_X$ is the number of
lenses the volunteer has been shown, while $N_{``X"}$ is the number of 
times the volunteer got their classifications of this type of training subject
right. $T$ stands for all
$N_{\LENS} + N_{\NOT}$ training data that the agent has heard about to
date. 

The full confusion matrix of the $k^{\rm th}$ volunteer's agent is therefore:
\be
  \mathcal{M}^k = 
  \begin{bmatrix}
    \pr(\saidLENS|\NOT,T_k) & \pr(\saidLENS|\LENS,T_k) \\
    \pr(\saidNOT |\NOT,T_k) & \pr(\saidNOT |\LENS,T_k)
  \end{bmatrix}.
\ee
Note that these probabilities are normalized, such that
$\pr(\saidNOT |\NOT) = 1 - \pr(\saidLENS|\NOT)$.

Now, when this volunteer views a test subject, 
it is this confusion matrix that will allow their agent to update the
probability of that test subject being a $\LENS$. Let us suppose that
this subject has never been seen before: the agent assigns a 
prior probability that it is (or contains) a lens is 
\be
  \pr(\LENS) = p_0
\ee
where we have to assign a value for $p_0$. In the CFHTLS, we might expect
something like 100 lenses in 430,000 images, so $p_0 = 2\times10^{-4}$
is a reasonable estimate. The volunteer then makes a classification $C_k$ 
($= \saidLENS$ or $\saidNOT$).
We can apply Bayes' Theorem to derive how the agent should
update this prior probability into a posterior one using this new information:
\begin{align}
  \label{eq:app:first}
  & \pr(\LENS|C_k,T_k) = \\
  & \frac{\pr(C_k|\LENS,T_k)\cdot\pr(\LENS)}
{\left[ \pr(C_k|\LENS,T_k)\cdot\pr(\LENS) + \pr(C_k|\NOT,T_k)\cdot\pr(\NOT) \right]},
  \notag
\end{align}
which can be evaluated numerically using the elements of the confusion
matrix. 

As an example, suppose we have a volunteer who is always right about the true
nature of a training subject. 
Their agent's confusion matrix would be
\be
  \mathcal{M}^{\rm perfect} = 
  \begin{bmatrix}
    0.0 & 1.0 \\
    1.0 & 0.0
  \end{bmatrix}.
\ee
On being given a fresh subject that actually is a $\LENS$,
this hypothetical volunteer would submit $C = \saidLENS$. 
Their agent would then calculate the posterior probability for the 
subject being a $LENS$ to be
\begin{align}
  \pr(\LENS|\saidLENS,T_k) &= \frac{1.0 \cdot p_0}
           {\left[ 1.0\cdot p_0 + 0.0\cdot(1 - p_0) \right]}
   &= 1.0,
\end{align}
as we might expect for such a {\it perfect} classifier. 
Meanwhile, a hypothetical volunteer who (for some reason) 
wilfully always submits the wrong 
classification would have an agent with the column-swapped confusion matrix
\be
  \mathcal{M}^{\rm obtuse} = 
  \begin{bmatrix}
    1.0 & 0.0 \\
    0.0 & 1.0
  \end{bmatrix},
\ee
and would submit $C = \saidNOT$ for this subject. However, such a volunteer
would nevertheless be submitting useful
information, since given the above confusion matrix, their agent would
calculate
\begin{align}
  \pr(\LENS|\saidNOT,T_k) &= \frac{1.0 \cdot p_0}
           {\left[ 1.0\cdot p_0 + 0.0\cdot(1 - p_0) \right]}
   &= 1.0.
\end{align}
{\it Obtuse} classifiers are as helpful as {\it perfect} ones!

Indeed, the information content of each classification can be estimated by an
agent before the next classification is made, just from its confusion matrix.
We define the expected contributed information per classification, as
\be
  \langle I_k \rangle = \frac{1}{2} 
    \left(
    \mathcal{M}^k_{d}\cdot\log_2{\mathcal{M}^k_d} +  \mathcal{M}^k_{o}\cdot\log_2{\mathcal{M}^k_o} \right),
  \label{eq:app:info}
\ee
where $\mathcal{M}^k_d$ and $\mathcal{M}^k_o$ is the sum of the diagonal and
the off-diagonal elements of the confusion matrix and $\langle I_k \rangle$ is
measured in ``bits.'' The factor of 1/2 accounts for the way the confusion
matrix encodes the results for the classification, and
that the probabilities associated with each type are normalised.
\Eref{eq:app:info} gives the required result, that both the hypothetical {\it
perfect} and {\it obtuse} classifiers contribute 1 bit of information each, per
classification. 

In fact, the only classifier that contributes the minimum information per
classification, zero bits, is the {\it random} classifier, whose agent's
confusion matrix elements are all 0.5.   

We conservatively initialise all the agents' confusion matrices to be that of
the random classifier. This makes no allowance for volunteers that actually do
have previous experience of what gravitational lenses look like, but should
help prevent large numbers of false positives being assigned high probability.
%Plotting  $\langle I_k \rangle$ as a function of time will, to some extent,
%illustrate the
%learning process undergone by the $k^{\rm th}$ volunteer-agent partnership.

Suppose the $k+1^{\rm th}$ volunteer now submits a classification, on the same
subject just classified by the $k^{\rm th}$ volunteer. We can generalise
\Eref{eq:app:first} by replacing the prior probability with the current
posterior probability:
\begin{align}
  \label{eq:app:update}
  \pr(\LENS & |C_{k+1},T_{k+1},\data) = \\
  & \frac{1}{Z} \pr(C_{k+1}|\LENS,T_{k+1}) \cdot \pr(\LENS|\data) \\ \notag
{\rm where}\;\; Z = & \pr(C_{k+1}|\LENS,T_{k+1})\cdot\pr(\LENS|\data) \\ \notag
      & + \pr(C_{k+1}|\NOT,T_{k+1})\cdot\pr(\NOT|\data), \notag
\end{align}
and $\data = \{C_k,T_k\}$ is the set of all previous
classifications, and the set of training subjects seen by each of those
volunteers.
$\pr(\LENS|\data)$ is the fundamental property of each test subject that
we are trying to infer. We track $\pr(\LENS|\data)$ as a function of time,
and by comparing it to a lower or upper thresholds, make decisions about
whether to retire the subject from the classification interface or
promote it in \Talk, respectively.

The confusion matrix obtained from the application of Equation
(\ref{eq:app:fraction}) has some inherent noise which will reduce as the number
of training subjects classified by the user increases. For simplicity, the
discussion thus far assumed the case when the confusion matrix is known
perfectly. Let us now discuss the case when the noise in the confusion matrix
is known. For ease of notation, we will denote $\pr(C_k|\LENS,T_k)\equiv p_1$
and $\pr(C_k|\NOT,T_k)\equiv p_2$. In reality, there is a probability
distribution for both $p_1$ and $p_2$. Let $p_0$ be the prior probability of
the subject being a lens. Then the posterior probability, $p_0'$ of the subject
being a lens after the classification $C_k$ is
\be
  \label{eq:app:sec}
p_0' = \frac{p_1 p_0}{\left[ p_1 p_0 + p_2 (1- p_0) \right]},
\ee
In principle, the probability distributions for every agent $p_1,p_2$ and the
prior probability $p_0$ could be maintained on a fine grid, so that the full
posterior probability distribution can be obtained by the following
marginalization procedure:
\be
P(p_0') = \int p_0' P(p_1) P(p_2) P(p_0) dp_1 dp_2 dp_0
\ee
This marginalization is likely not tractable analytically, but can perhaps be
implemented in a Monte-Carlo fashion. This needs to be further investigated.

Instead, for simplicity let us make some approximations to reduce each of the
probability distribution to two numbers its mean and a variance. Let us assume
that $p_1$ and $p_2$ are Gaussian distributed around some mean value
$\bar{p}_x$ with a variance equal to $\sigma_x$. We will obtain $\bar{p}_0'$ by
substituting the mean values $\bar{p}_1$ and $\bar{p_2}$ in Equation
(\ref{eq:app:sec}) and the variance of $p_0'$ by simple error propagation
\begin{eqnarray}
\left(\frac{\sigma_0'}{\bar{p}_0'}\right)^2 =&& \left(\frac{\sigma_1}{p_1}\right)^2 + \left(\frac{\sigma_0}{p_0}\right)^2 \nonumber \\
&+& \frac{ (p_1\sigma_0)^2 + (\sigma_1p_0)^2 + \sigma_2^2 + (p_2\sigma_0)^2 + (\sigma_2p_0)^2}{ \left[ p_1 p_0 + p_2 (1- p_0)\right]^2 } \nonumber \\
\end{eqnarray}

Finally, we also need to update the confusion matrix of an agent and obtain the
variance on each element of the matrix, once a training subject has been
classified. We would like to derive the posterior probability of the
probability elements $p_1$ and $p_2$ given their prior probabilities. For this
purpose, we can again make use of Bayes' theorem,
\begin{equation}
P(p_x'|N_{"X"},N_X,T) = \frac{P(N_{``X"}|p_x',N_X,T) P(p_x'|N_X,T)}{\sum_{N_{``X"}} P(N_{``X"}|p_x',N_X,T) P(p_x'|N_X,T)}
\end{equation}
We assume that $P(N_{``X"}|p_x',N_X,T)$ is a Poisson distribution, although
this is not true strictly speaking given that our agents are learning and the
values of the confusion matrix are moving. Modelling the learning curve of our
users is yet another complicated extension we could think about.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%  REFERENCES
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% MNRAS does not use bibtex, input .bbl file instead. 
% Generate this in the makefile using bubble script in scriptutils:

% bubble -f paper-lcr.tex references.bib 
% \input{paper-lcr.bbl}

\bibliographystyle{apj}
\bibliography{references}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\label{lastpage}
\bsp

\end{document}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
