%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% Space Warps System Paper
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\documentclass[useAMS,usenatbib,a4paper]{mn2e}
%% letterpaper
%% a4paper

\voffset=-0.6in

% Packages:
\input psfig.sty
\usepackage{xspace}
\usepackage{graphicx}
\usepackage{amssymb}
\usepackage{amsmath}

% Macros:
\input{macros.tex}
\input{addresses.tex}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\title[\sw]
{\SW: Crowd-sourcing the Discovery of Gravitational Lenses}
    
\author[Marshall et al.]{%
  \input{sw-system-authors.tex}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}
             
\date{to be submitted to MNRAS}
\pagerange{\pageref{firstpage}--\pageref{lastpage}}\pubyear{2014}

\maketitle           

\label{firstpage}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{abstract} 

\sw is a web-based service that enables the discovery of strong gravitational
lenses in wide-field imaging surveys by large numbers of people. Carefully
produced color composite images are displayed to volunteers via a
classification interface which records their estimates of the positions of
candidate lensed features. Simulated lenses, and expert-classified non-lenses,
are inserted into the image stream at random intervals; this training set is
used to give the volunteers feedback on their performance, and to estimate a
dynamically-updated probability for any given image to contain a lens. Low
probability systems are retired from the site periodically, concentrating the
sample towards a set of candidates; this ``stage 1'' set is then re-classified
by the volunteers in a second  refinement stage. Analyzing the classification
of the training set, we predict that the first stage alone should yield a
sample that is C\% complete, while leading to the rejection of R\% of the
initial target sample. Having divided the 150 square degree CFHTLS imaging
survey into 430000 overlapping 70 by 70 arcminute tiles and displayed them on
the site, we were joined by 33000 volunteers who contributed X million image
classifications over the course of N months. The sample was reduced to 3500
stage 1 candidates; these were then refined to yield a sample of 1400
candidates rankable by their stage 2 probability. We expect this sample to be
X\% complete and Y\% pure at a threshold of 95\% classification probability.
We find that, on average, and given the assumptions we make in our analysis,
we need 9 classifications per image during the first stage, X in the second.
We estimate the mean information contributed per person to be X bits, over a
session lasting, on average, N classifications per volunteer, and present the
highly skewed distributions of these quantities. We comment on the scalability
of the \sw system to the wide field survey era, and its potential to operate
beyond its design as a supervised classification system. 

\end{abstract}

% Full list of options at http://www.journals.uchicago.edu/ApJ/instruct.key.html

\begin{keywords}
  gravitational lensing   --
  methods: statistical    --
  methods: citizen science
\end{keywords}

\setcounter{footnote}{1}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Introduction}
\label{sec:intro}

% Scientific motivation. Applications of lenses: group-scale arcs,
% galaxy-galaxy lenses, lensed quasars. 

Strong gravitational lensing -- the formation of multiple, magnified images of
background objects due to the deflection of light by  massive foreground
objects -- is a very powerful astrophysical tool, enabling a wide range of
science projects. The image separations and distortions provide information
about the mass distribution in the lens \citep[e.g.][]{AugerEtal2010,
SonnenfeldEtal2012,SonnenfeldEtal2013}, including on sub-galactic scales
\citep[e.g.][]{Dalal+Kochanek2002,VegettiEtal2010,HezavehEtal2013}. Any
strong lens can provide magnification of a factor of 10 or more, providing a
deeper, higher resolution view of the distant universe through these ``cosmic
telescopes'' \citep[e.g.][]{StarkEtal2008,NewtonEtal2011}. Lensed quasars enable
cosmography via the time delays between the multiple images' lightcurves
\citep[e.g.][]{TewesEtal2013,SuyuEtal2013}, and study of the accretion disk itself
through the microlensing effect \citep[e.g.][]{PoindexterEtal2008}. All of these
science projects would benefit from being able to draw from a larger sample of
lenses.

In the last decade the numbers of detections of these rare cosmic alignments
has increased by an order of magnitude, thanks to wide field surveys such as
CLASS \citep[e.g.]{BrowneEtal2003}, SDSS \citep[e.g.][]
{BoltonEtal2006,AugerEtal2010b,TreuEtal2011,InadaEtal2012}, CFHTLS
\citep[e.g.][]{MoreEtal2012,GavazziEtal2014}, Herschel
\citep[][]{NegrelloEtal2014} and SPT \citep[e.g.][]{VieiraEtal2013}, among
others.  As the number of known lenses has increased, new types have been
discovered, leading to entirely new investigations. Compound lenses
\citep{GavazziEtal2008,CollettEtal2012} and lensed supernovae
\citep{QuimbyEtal2014} are good examples of this. 

% Problem of rarity. Imaging surveys. Problem of purity/false positives.
% Review of progress to date. Methods in SL2S, SQLS. Contrast with SLACS. 

Because they are rare, strong lenses are expensive to find. The most efficient
searches to date have made use of relatively clean signals such as the
presence of emission or absorption features at two distinct redshifts in the
same optical spectrum \citep[e.g.][]{BoltonEtal2004}, or the strong
``magnification bias'' towards detecting strongly-lensed sources in the sub-mm
waveband \citep[e.g.][]{NegrelloEtal2010}. Such searches have to be efficient,
because they require expensive high resolution imaging follow-up; consequently
they have so far produced yields in the tens to hundreds. An alternative approach
is to search images of sufficiently high resolution and color contrast, and
confirm the systems as gravitational lenses by modeling the survey data
themselves \citep[][]{MarshallEtal2009}. Several square degrees of HST images
have been searched, yielding several tens of galaxy-scale lenses
\citep[e.g.][]{MoustakasEtal2007,FaureEtal2008,Jackson2008,MoreEtal2012,
PawaseEtal2014}.
Similarly, searches of over a hundred square degrees of CFHT Legacy Survey
ground-based imaging, also with sub-arcsecond image quality, have revealed a
smaller number of wider image separation group-scale systems
\citep[e.g.][]{CabanacEtal2007,MoreEtal2012}. Detecting galaxy-scale lenses from
the ground is hard, but feasible albeit lower efficiency and requiring HST or
spectroscopic follow-up to confirm the candidates as lenses
\citep[e.g.][]{GavazziEtal2014}.

% Scaling to wide field era. Automated methods: problems. Need for good
% training sets. Need for quality control: always present.

How can we scale these lens searches up to imaging surveys covering a hundred
times the sky area, such as the almost-all sky surveys planned with LSST and
Euclid, while reducing our dependence on expensive follow-up confirmation
observations? There are two approaches to detecting lenses in imaging surveys.
The first one is robotic: automated analysis of object catalogs and/or the
survey images. The candidate samples produced by these methods have, to date,
not been of high purity  \citep[see
e.g.][]{MarshallEtal2009,MoreEtal2012,GavazziEtal2014}, with visual inspection by
teams of humans still required to narrow down the robotically-generated
samples. In this approach, the image data may or may not be explicitly
modelled by the robots as if it contained a gravitational lens, but the visual
inspection can be thought of as a ``mental modeling'' step. Systems classified
by an inspector to be good lens candidates are deemed as such because the
features in the image can be explained by a model of what gravitational lenses
do contained in the inspector's brain. The second approach simply cuts out the
robot middleman: \citet{FaureEtal2008,Jackson2008} and \citet{PawaseEtal2014}
all performed entirely visual searches for lenses in HST imaging.

Visual image inspection seems, at present, unavoidable at some level when
seraching for gravitational lenses. The technique has some drawbacks, however.
First is that humans are only humans, and they make mistakes. The solution to
this is to operate in teams, providing multiple classifications of the same
images in order to catch errors and correct them. Second, and relatedly, is
that humans get tired. With a well-designed classification interface, a human
might be able to inspect images at a rate of one astronomical object per
second (provided the majority are indeed uninteresting). At $10^4$ massive
galaxies, and 10 lenses, per square degree, visual lens searches in good
quality imaging data are limited to a few square degrees per inspector per
day. Scaling to thousands of square degrees therefore means either robotically
reducing the number of targets for inspection, or increasing the number of
inspectors, or both. 

For example, a $10^4$ square degree survey containing $10^8$
photometrically-selected massive galaxies and $10^5$ lenses could only be
searched by 10 inspectors at a mean rate of 1 galaxy per second and 10
inspections per galaxy in about 14 years. Reducing the inspection time by a
factor of 400 to two weeks would require a robot to reduce the target sample
to 25 per square degree. However, at this point the required purity, 40\%,
would very likely require the average classification time per object to be
more like 10 seconds per object. Hiring 10 inspectors to assess complex images
full time full time for five months may not be the most cost-effective or
reliable strategy. Alternatively, a team of $10^6$ inspectors could, in
principle, make the required $10^9$ image classifications, $10^3$ each, in a
few weeks; robotically reducing the target list would lead to a proportional
decrease in the required team size.

Systematic detection of rare astronomical objects by such ``crowd-sourced''
visual inspection has recently been achieved by the online citizen science
project PlanetHunters \citep{SchwambEtal2012}. PlanetHunters was designed to
enable the discovery of transiting exoplanets in data taken by the Kepler
satellite; a community of N inspectors from the general public found, after
each undergoing a small amount of training, N new exoplanet candidates by
visual inspection of the Kepler lightcurves that were presented in a custom
web-based classification interface. The older Galaxy Zoo morphological
classification project \citep{LintottEtal2008} has also enabled the discovery
of rare objects, via its flexible inspection interface and discussion forum
\citep{LintottEtal2009}. Indeed, several of us (AV,EB,CC,TJ,CM,LW) were active
in an informal Galaxy Zoo gravitational lens search, an experience which led
to the present hypothesis that a systematic online visual lens search could be
successful. 

In this paper, we describe the \sw website, an online system that enables 
crowd-sourced gravitational lens detection by inviting volunteers to classify
astronomical survey images as containing lens candidates or not.  In a
companion paper we will present the new gravitational lenses discovered in our
first experimental lens search, and begin to investigate the differences
between lens detections made in \sw and those made with automated techniques.
Here though, we try to answer the following questions:

\begin{itemize}

\item How reliably can we find gravitational lenses using the \sw
system? What is the completeness of the sample produced?

\item How noisy is the system? What is the purity of the sample
produced?

\item How quickly can lenses be detected, and non-lenses be rejected?
How many classifications, and so how many volunteers are needed per target?

\item What can we learn about the scalability of the crowd-sourcing approach?

\end{itemize}

This paper is organised as follows.  In \Sref{sec:design} we introduce the \sw
classification interface and the volunteers who make up the \sw
collaboration,  explain how we use training images, and describe our two stage
candidate selection strategy. We then briefly introduce, in \Sref{sec:data}
the particular dataset used in the first experimental tests of the \sw system,
and how we prepared the images prior to displaying them in the web interface.
In \Sref{sec:swap} we describe our methodology for interpreting the
classifications made by the volunteers, and then present the results of system
performance tests  made on the training images in \Sref{sec:results}. % In
this section, we also investigate the properties of the crowd, to % understand
where the information is coming from. We discuss the implications of our
results for future lens searches in \Sref{sec:discuss} and draw conclusions in
\Sref{sec:conclude}.

% \section{Introduction}
% \label{sec:intro}
% \section{Experiment Design}
% \label{sec:design}
% \subsection{Training}
% \label{sec:design:training}
% \subsection{Stage 1: Initial Classification}
% \label{sec:design:stage1}
% \subsection{Stage 2: Refinement}
% \label{sec:design:stage2}
% \section{Data}
% \label{sec:data}
% \subsection{The CFHT Legacy Survey}
% \label{sec:data:CFHTLS}
% \subsection{Image Presentation}
% \label{sec:data:display}
% \section{Classification Analysis}
% \label{sec:swap}
% \section{Results}
% \label{sec:results}
% \subsection{Crowd Properties}
% \label{sec:results:crowd}
% \subsection{Sample completeness and purity}
% \label{sec:results:sample}
% \section{Discussion}
% \label{sec:discuss}
% \section{Conclusions}
% \label{sec:conclude}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Experiment Design}
\label{sec:design}

The basic steps of a visual search for gravitational lenses are: 1) prepare
images, 2) display them to an inspector, 3) record the inspector's
classification of each image (as, for example, containing a lens candidate or
not) and 4) analyzing those classifications (and all others) in order to
produce a final candidate list. We describe step 1 in \Sref{sec:data} and step
4 in \Sref{sec:swap}. In this section we take a volunteer's eye view and
begin by describing the \sw classification interface, the crowd of volunteers,
and the interactions between the two.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure*}
\centerline{
\includegraphics[width=0.9\linewidth]{sw-system-figs/sw-screengrab-marker+feedback.png}}
\caption{Screenshot of the \sw classification interface.}
\label{fig:screenshot}
\end{figure*}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % 

\subsection{Classification Interface}
\label{sec:design:interface}

A screenshot of the \sw classification interface (CI) is shown in
\Fref{fig:screenshot}. The CI is the centrepiece of the \sw website,
\texttt{http://spacewarps.org}; the web application is written in
coffeescript, css and html and follows the general design of others written by
the Zooniverse team.\footnote{The \sw web application code is open source and
can be accessed from \texttt{https://github.com/zooniverse/Lens-Zoo}}  The
focus of the CI is a large display of the current pre-prepared PNG image of
the ``subject'' being inspected.   When the image is clicked on by the
volunteer, a marker symbol appears where the pointer was. Several markers can
be placed.  The next image moves rapidly in from a queue formed at the right
hand side of the screen when the ``Finished marking'' button is pressed. At
the same time, the positions of the markers are written out to the
classification database, in an entry that also stores the ID of the subject,
the username (or IP address) of the volunteer, a timestamp and some other
metadata. 

Gravitational lenses are rare: typically, most of the images will not contain
a lens candidate, and these need to be quickly rejected by the inspector. The
queue allows several images to be pre-loaded while the volunteer is
classifying the current subject, and the rapid movement is designed to
encourage volunteers to classify rapidly.

For the more interesting subjects, the CI offers two features that  enable
further investigation of the subjects. First is the ``Quick Dashboard'' (QD) a
more advanced image viewer. This allows the viewer to compare three different
contrast and color balance settings, to help bring out subtle features, and to
zoom in on interesting regions of the image to assess small features. Markers
can be placed in the Quick Dashboard just the same as in the main CI image
viewer. The second is a link to that subject's page in the project discussion
forum, \texttt{http://talk.spacewarps.org}. Here, volunteers can discuss the
features they have seen either before they submit their classification, or
after, if they ``favorite'' the subject. There is no ``back'' button: each
volunteer may only classify a given subject once. However, the presence of an
option to see what others think about any given subject before submitting your
own classification means that the classifications may not be  strictly
independent; the advantage of this system is that volunteers can learn from
others what constitutes a good lens candidate. In practice, we might expect
this to be a relatively unimportant educational resource, given the explicit
training we provide for the volunteers, which we describe in the next section.


% % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % 

\subsection{Training}
\label{sec:design:training}

Gravitational lenses are unfamiliar objects to volunteers who are new
to the site. New volunteers need to learn what lenses look like as quickly as
possible, so that they can contribute informative classifications. They also
need to learn what lenses do not look like, in order to reduce the false
positive detection rate. There are three primary mechanisms in the \sw system
for teaching the volunteers what to look for. These are, in the order in which
they are encountered, an inline tutorial, instant feedback,
and a ``Spotter's Guide.''

\subsubsection{Inline Tutorial}

New volunteers are welcomed to the site with a very short tutorial, in which
the task is introduced, a typical image containing a simulated lens is
displayed, and the marking procedure walked through, using pop-up message
boxes. Subsequent images gradually introduce the more advanced
features of the classification interface (the QD and Talk buttons), also using
pop-up messages. The tutorial was purposely kept as short as possible so as to
provide the minimal barrier to entry.

\subsubsection{Instant Feedback}

The second image viewed after the initial tutorial image is already a survey
image, in order to get the volunteers engaged in the real task as quickly as
possible. Training continues beyond the first image tutorial through
``training subjects'' inserted randomly into the stream.  These training
subjects are either simulated lenses (known as ``sims''), or survey images
that were expert-classified (by AV, AM and PM) and found not to contain any
lens candidates (these images are known as ``duds''). The tutorial explains
that the volunteers  will be shown such training images. They are also
informed that they will receive instant feedback about their performance after
classifying (blind) any of these training subjects. Indeed, after a volunteer
finishes marking a training subject and hits ``Finished marking,'' a pop-up
message is generated, containing either positive feedback for a successful
classification (for example, ``Well done! You spotted a simulated lens,'' as
in \Fref{fig:screenshot}) or negative feedback for an unsuccessful one (for
example, ``There is no gravitational lens in this field!'') 

\question{PJM}{Does the feedback lead to volunteers leaving the site more 
quickly, or less quickly? Does negative feedback have more impact than
positive?}

The initial frequency of the training images is set to be two in five;
subjects are drawn randomly from the pool of training images with this
frequency. The pool contains equal numbers of sims and duds, and the draw is
made without replacement (for that volunteer). As the number of
classifications made by a volunteer increases, this frequency is decreased, to
$2/(5\times2^{(\textrm{int}(N_c/20)+1)/2})$ ($\approx 0.3$ for the second 20
subjects, $0.2$ for the third 20 subjects, and so on).

This training regime means that in the first 60 images viewed, each volunteer
is shown (on average) 9 simulated gravitational lenses, and 9 empty fields. 
This is a much higher rate than the natural one: to try and avoid this leading
to over-optimism among the inspectors (and a resulting high false positive
rate), we display the current ``Simulation Frequency'' on the classification
interface (``1 in 5'' in \Fref{fig:screenshot}) and maintain the consistent
theme in the feedback messages that lenses are rare.


\subsubsection{Spotter's Guide}

Learning what lenses do: Spotter's Guide. Learning how
lenses work (science page, FAQ). 


% % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % 

\subsection{Stage 1: Initial Classification}
\label{sec:design:stage1}

Interface fast due to pre-loading of images, and minimizing interaction.
Trade-off between speed and accuracy. Decreasing training rate.

Quick dashboard provides simple ways to explore further: zoom, contrast
controls.

Spotting lenses: Markers to be placed. Two reasons: first, to give good
feedback. Second, to focus attention.

Non-lenses marked? Favourite button instead, enabling serendipitous
discovery of other interesting things, separate from lenses.

Retirement of low probability systems. Concentrates sample, provides more
``bacon'' (while slightly skewing "sim frequency"). Note that this feature
means that everyone contributes to detection of lenses: luck is made for the
few that happen to see the new lenses, by the masses that did the rejection.
Group effort.

Sims vs duds leads to inclusive search -- click on anything you think etc...

% % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % 

\subsection{Stage 2: Refinement}
\label{sec:design:stage2}

Goal: assess candidates, reject false positives by comparing with training set
of non-obvious non-lenses. Produce a sample rankable by probability.

Reconfigured website: more detailed SG, more detailed feedback. Orange
background to make it obvious stage 2 is different. Slower image presentation.
Higher, constant training rate.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Data}
\label{sec:data}

Definitions: training subjects and test subjects. Sims and duds.

% % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % 

\subsection{The CFHT Legacy Survey}
\label{sec:data:CFHTLS}

Describe survey. Refs. 

Why this one? Good IQ, deep, colorful, homogeneous. Precursor to Stage III and
IV imaging surveys, DES, KIDS, LSST etc. Already searched by robots: enables
comparison of techniques. Lenses not yet found by robots, detectable by
humans? 

Blind search strategy.
Preparation of data: divide survey into overlapping tiles. 


% % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % 

\subsection{Image Presentation}
\label{sec:data:display}

Presentation of images. Uniform scales, to build intuition and avoid rescales
due to bright objects. Arcsinh stretch, to bring out low SB features. 
Approximately optimized, how? Examples of images.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Classification Analysis}
\label{sec:swap}

In this section we outline our methodology for interpreting the interactions
of the volunteers with the identification interface.  Each classification made
is logged in a database, storing subject IDs, (anonymous) volunteer IDs, a
timestamp and the classification results.  The {\it kind} of subject --
whether it is a training subject (a  simulated lens or a known non-lens) or a
test subject (an unseen image drawn from the survey) -- is also recorded. For
all subjects, the positions of all Markers are recorded, in pixel coordinates.
For training subjects, we also store the ``classification'' of the subject as
a lens, or a non-lens, and also the type of object present in the image. These
types are summarized in \Tref{tab:objecttypes}.  This classification is used
to provide instant feedback, but is also the basic measurement used in a
probabilistic classification of every subject based on all image views to
date.

We perform an online analysis of the classifications,  updating a
probabilistic model of every (anonymous) volunteer's data, and also updating
the lens probability of each subject  (in both the training and test sets), on
a daily basis. This gives us a dynamic estimate of the posterior probability
for  any given  subject being a lens, given all classifications of it to date.
Assigning thresholds in this lens probability then allows us to make good
decisions about whether or not to retire a subject from the system, in order to
focus attention on new images. 

The details of how the lens probabilities are calculated are given in
\Aref{appendix:swap}. In summary:
\begin{itemize}

\item Each volunteer is assigned a simple software agent, characterised by a
confusion matrix. The two independent elements of this matrix are the
probabilities, as estimated by the agent, that the volunteer is going to be 1)
correct when they report that an image contains a lens when it really does
contain a lens, $\pr(\saidLENS|\LENS,T)$, and 2) correct when they report that
an image does not contain a lens when it really doesn't contain a lens,
$\pr(\saidNOT|\LENS,T)$.

\item Each agent updates its confusion matrix elements based on the number of
times its volunteer has been right in each way while classifying subjects from
the training set, accounting for noise early on due to small number
statistics: $T$ is the set of all training images seen to date.

\item Each agent uses its confusion matrices to update, via Bayes' theorem,
the probability of an image from the test set containing a lens,
$\pr(\LENS|C,T)$, when that image is classified by its volunteer. ($C$ is the
set of all classifications made of this subject.)

\end{itemize}

In the next section we present results in terms of all three of these
probabilities, as we investigate the performance of the \SW system.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Results}
\label{sec:results}

Understanding crowd, so we can help them learn faster. Understanding
images given the crowd, so we can find lenses.

% % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % 

\subsection{Crowd Properties}
\label{sec:results:crowd}

Enthusiasm: histogram of classifications, stage 1 vs stage 2. Information
contributed. Correlation with number of classifications. 

PL and PD as measures of skill. not quite talent, due to possibility of
learning - but agents assume talent. Performance of crowd re PD and PL.
Correlations with N classifications.


% % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % 

\subsection{Sample completeness and purity}
\label{sec:results:sample}

Rejection rate. Completeness and purity at P > retirement, P > 95\%, and 
as function of probability P. Compare stage 1 and stage 2. 

Summarize performance at some fiducial threshold: eg P = 95\%.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Discussion}
\label{sec:discuss}

Challenges for future.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Conclusions}
\label{sec:conclude}

Summary of system.

Crowd-sourced gravitational lens detection works, in terms of the
classification of the training set as described here, in the following
specific ways: 

\begin{itemize} 

\item Participation (crowd size, activity rate) enabled project completion

\item Both stages (1 and 2) achieved the required rejection rates

\item Integrated humanpower = X (stage 1) and y (stage 2), cf hours taken by
small team of experts 

\item Nightly processing is inefficient: more classifications were made than
was necessary during peak participation. Need kafka...

\item Retirement rate. False negatives: which sims were missed?

\item The optimal true positive rate (completeness) and false positive rate in
the training set were estimated to be TPR\% and FPR\% at Stage 1, assuming a
detection threshold of xxx.  

\item In the ``refinement'' stage 2, X\% of the stage 1 candidates were
rejected (with P less than threshold), and the remainder assigned lens
``probabilities.'' Ranking
subjects by their Stage 2 lens probability gives an ROC curve for the system
with X properties. The optimal true positive rate (completeness) and false
positive rate in the training set were estimated as TPR\% and FPR\%; 

\item The lens-finding crowd shows some interesting properties, with
consequences for future scalability

\item The information comes predominantly from volunteers with agents with P =
...

\item The agents show a high mean information per classification, which
increased/decreased with time; this does/doesn?t correlate with active crowd
size, showing how the crowd changed over time...

\end{itemize}

Sum up, end.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%  ACKNOWLEDGMENTS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section*{Acknowledgements}
 
\input{acknowledgments.tex}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%  APPENDICES
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\appendix

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Probabilistic Classification Analysis}
\label{appendix:swap}

Our aim is to enable the construction of a sample of good lens candidates.
Since we aspire to making logical  decisions, we define a  ``good candidate''
as one which has a high posterior probability of being a lens, given the data:
$\pr(\LENS|\data)$. Our problem is to approximate this probability. The data~$\data$
in our case are the pixel values of a colour image. However, we can greatly
compress these complex, noisy sets of data by asking each volunteer what they
think about them. A complete  classification in \sw consists of a set of
Marker positions, or none at all. The null set encodes the statement from
the volunteer that the image in question is $\saidNOT$ a lens, while the
placement of any  Markers indicates that the volunteer considers this image to
contain a $\saidLENS$.  We simplify the problem by only using the Marker
positions to assess whether the volunteer  correctly assigned the
classification $\saidLENS$ or $\saidNOT$ after viewing (blindly) a member of
the training set of subjects. 

How should we model these compressed data? The circumstances of each
classification are quite complex, as are the human classifiers in general: the
volunteers learn more about the problem as they go, but also inevitably make
occasional mistakes (perhaps because a lens is difficult to see, or they
became distracted during the task). To cope with this uncertainty, we assign a
simple software {\it agent} to partner each volunteer. The agent's task is to
interpret their volunteer's classification data as best it can, using a model
that makes a number of necessary approximations. These interpretations will
then include uncertainty arising as a result of the volunteer's efforts and
also the agent's approximations, but they will have two important redeeming
features. First, the interpretations will be quantitative (where before they
were qualititative),  and thus will be useful in decision-making. Second, the
agent will be able to predict, using its model, the probability of a test
subject being a $\LENS$, given both its volunteer's classification, and its
volunteer's experience. In this appendix we describe how these agents work.


\subsection{Agents and their Confusion Matrices}
\label{appendix:swap:probabilities}

Each agent assumes that the probability of a volunteer recognising any given
simulated lens as a lens is some number, $\pr(\saidLENS|\LENS,T)$, that
depends only on what the volunteer is currently looking at, and all the
previous training subjects they have seen (and not on what type of lens it is,
how faint it is, what time it is, \etc). Likewise, it also assumes that the
probability of a volunteer recognising any given dud image as a dud is some
other number, $\pr(\saidNOT|\NOT,T)$, that also depends only on what the volunteer is currently looking at, and all the
previous training subjects they have seen. These two probabilities define a 
2 by 2 ``confusion matrix,'' which the agent updates, every time a
volunteer classifies a training subject, using the following 
very simple estimate:
\be
  \pr(``X"|X,T) \approx \frac{N_{``X"}}{N_X}.
  \label{eq:app:fraction}
\ee
Here, $X$ stands for the true classification of the subject, \ie either
$\LENS$ or $\NOT$, while $``X''$ is the corresponding classification
made by the volunteer on viewing the subject. $N_X$ is the number of
lenses the volunteer has been shown, while $N_{``X"}$ is the number of 
times the volunteer got their classifications of this type of training subject
right. $T$ stands for all
$N_{\LENS} + N_{\NOT}$ training data that the agent has heard about to
date. 

The full confusion matrix of the $k^{\rm th}$ volunteer's agent is therefore:
\be
  \mathcal{M}^k = 
  \begin{bmatrix}
    \pr(\saidLENS|\NOT,T_k) & \pr(\saidLENS|\LENS,T_k) \\
    \pr(\saidNOT |\NOT,T_k) & \pr(\saidNOT |\LENS,T_k)
  \end{bmatrix}.
\ee
Note that these probabilities are normalized, such that
$\pr(\saidNOT |\NOT) = 1 - \pr(\saidLENS|\NOT)$.

Now, when this volunteer views a test subject, 
it is this confusion matrix that will allow their agent to update the
probability of that test subject being a $\LENS$. Let us suppose that
this subject has never been seen before: the agent assigns a 
prior probability that it is (or contains) a lens is 
\be
  \pr(\LENS) = p_0
\ee
where we have to assign a value for $p_0$. In the CFHTLS, we might expect
something like 100 lenses in 430,000 images, so $p_0 = 2\times10^{-4}$
is a reasonable estimate. The volunteer then makes a classification $C_k$ 
($= \saidLENS$ or $\saidNOT$).
We can apply Bayes' Theorem to derive how the agent should
update this prior probability into a posterior one using this new information:
\begin{align}
  \label{eq:app:first}
  & \pr(\LENS|C_k,T_k) = \\
  & \frac{\pr(C_k|\LENS,T_k)\cdot\pr(\LENS)}
{\left[ \pr(C_k|\LENS,T_k)\cdot\pr(\LENS) + \pr(C_k|\NOT,T_k)\cdot\pr(\NOT) \right]},
  \notag
\end{align}
which can be evaluated numerically using the elements of the confusion
matrix. 

\subsection{Examples}
\label{appendix:swap:examples}

Suppose we have a volunteer who is always right about the true
nature of a training subject. 
Their agent's confusion matrix would be
\be
  \mathcal{M}^{\rm perfect} = 
  \begin{bmatrix}
    0.0 & 1.0 \\
    1.0 & 0.0
  \end{bmatrix}.
\ee
On being given a fresh subject that actually is a $\LENS$, this hypothetical
volunteer would submit $C = \saidLENS$.  Their agent would then calculate the
posterior probability for the subject being a $LENS$ to be
\begin{align}
  \pr(\LENS|\saidLENS,T_k) &= \frac{1.0 \cdot p_0}
           {\left[ 1.0\cdot p_0 + 0.0\cdot(1 - p_0) \right]}
   &= 1.0,
\end{align}
as we might expect for such a {\it perfect} classifier.  Meanwhile, a
hypothetical volunteer who (for some reason) wilfully always submits the wrong
classification would have an agent with the column-swapped confusion matrix
\be
  \mathcal{M}^{\rm obtuse} = 
  \begin{bmatrix}
    1.0 & 0.0 \\
    0.0 & 1.0
  \end{bmatrix},
\ee
and would submit $C = \saidNOT$ for this subject. However, such a volunteer
would nevertheless be submitting useful information, since given the above
confusion matrix, their agent would calculate
\begin{align}
  \pr(\LENS|\saidNOT,T_k) &= \frac{1.0 \cdot p_0}
           {\left[ 1.0\cdot p_0 + 0.0\cdot(1 - p_0) \right]}
   &= 1.0.
\end{align}
{\it Obtuse} classifiers turn out to be as helpful as {\it perfect} ones.


\subsection{Information Contribution}
\label{appendix:swap:examples}

The information likely to be contributed by each agent for a given subject
can be estimated before the next classification of that subject is made, 
just from its confusion matrix.
The Shannon entropy generated by a classifier upon performing a
classification is
\be
  \langle S_k \rangle =  
   -P_{\rm right}\cdot\log_2{P_{\rm right}} -  P_{\rm wrong}\cdot\log_2P_{\rm wrong}\,,
\ee
where $P_{\rm right}$ and $P_{\rm wrong}$ are the averages of the diagonal and
the off-diagonal elements of the confusion matrix, respectively, and $\langle
S_k \rangle$ is measured in ``bits.'' These averages represent the probability
of a classifier to get a classification right or wrong, respectively. We define
the information contributed by a classifier as
\begin{equation}
I_k = 1 - S_k\,.
  \label{eq:app:info}
\end{equation}
\Eref{eq:app:info} gives the required result, that both the hypothetical {\it
perfect} and {\it obtuse} classifiers contribute 1 bit of information each, per
classification. Classifiers whose agent's confusion matrix is such that $P_{\rm
right}=P_{\rm wrong}=0.5$, contribute zero bits of information. Such users
identify a lens correctly with the same probability as they misclassify a dud
image to contain a lens, and thus their classification is of no value. 

We conservatively initialise all the elements of the agents' confusion matrices
to be 0.5, that of a random classifier. This makes no allowance for volunteers
that actually do have previous experience of what gravitational lenses look
like, but should help prevent large numbers of false positives being assigned
high probability. Plotting  $\langle I_k \rangle$ as a function of time will,
to some extent, illustrate the learning process undergone by the $k^{\rm th}$
volunteer-agent partnership. 

\subsection{Updating the Subject Probabilities}
\label{appendix:swap:examples}

Suppose the $k+1^{\rm th}$ volunteer now submits a classification, on the same
subject just classified by the $k^{\rm th}$ volunteer. We can generalise
\Eref{eq:app:first} by replacing the prior probability with the current
posterior probability:
\begin{align}
  \label{eq:app:update}
  \pr(\LENS & |C_{k+1},T_{k+1},\data) = \\
  & \frac{1}{Z} \pr(C_{k+1}|\LENS,T_{k+1}) \cdot \pr(\LENS|\data) \\ \notag
{\rm where}\;\; Z = & \pr(C_{k+1}|\LENS,T_{k+1})\cdot\pr(\LENS|\data) \\ \notag
      & + \pr(C_{k+1}|\NOT,T_{k+1})\cdot\pr(\NOT|\data), \notag
\end{align}
and $\data = \{C_k,T_k\}$ is the set of all previous
classifications, and the set of training subjects seen by each of those
volunteers.
$\pr(\LENS|\data)$ is the fundamental property of each test subject that
we are trying to infer. We track $\pr(\LENS|\data)$ as a function of time,
and by comparing it to a lower or upper thresholds, make decisions about
whether to retire the subject from the classification interface or
promote it in \Talk, respectively.


\subsection{Uncertainty in the Agent Confusion Matrices}
\label{appendix:swap:uncertainty}

The confusion matrix obtained from the application of Equation
(\ref{eq:app:fraction}) has some inherent noise which reduces as the number of
training subjects classified by the agent's volunteer increases. For
simplicity, the discussion thus far assumed the case when the confusion matrix
is known perfectly; in practice, we allow for uncertainty in the agent
confusion matrices by averaging over a small number of samples drawn from
Binomial distributions characterised by the matrix elements
$\pr(C_k|\LENS,T_k)$ and  $\pr(C_k|\NOT,T_k)$. The associated standard
deviation in the estimated subject probability provides an error bar for this
quantity.

% For ease of notation, we will denote $\pr(C_k|\LENS,T_k)\equiv p_L$ and
% $\pr(C_k|\NOT,T_k)\equiv p_N$. In reality, there is a probability distribution
% for both $p_L$ and $p_N$. Let $p_0$ be the prior probability of the subject
% being a lens. Then the posterior probability, $p_0'$ of the subject being a
% lens after the classification $C_k$ is
% \be
%   \label{eq:app:sec}
% p_0' = \frac{p_L p_0}{\left[ p_L p_0 + p_N (1- p_0) \right]},
% \ee
% The posterior probability distribution $p_0'$ can be obtained by marginalizing
% over the probability distributions of $p_L$, $p_N$ and the prior probability
% distribution $p_0$ such that,
% \be
% P(p_0') = \int p_0' P(p_L) P(p_N) P(p_0) dp_L dp_N dp_0\,.
% \ee
% This marginalization is not analytically tractable. Therefore, we have
% implemented the following Monte-Carlo solution for this problem.
% 

%Finally, we also need to update the confusion matrix of an agent and obtain the
%variance on each element of the matrix, once a training subject has been
%classified. We would like to derive the posterior probability of the
%probability elements $p_1$ and $p_2$ given their prior probabilities. For this
%purpose, we can again make use of Bayes' theorem,
%\begin{equation}
%P(p_x'|N_{"X"},N_X,T) = \frac{P(N_{``X"}|p_x',N_X,T) P(p_x'|N_X,T)}{\sum_{N_{``X"}} P(N_{``X"}|p_x',N_X,T) P(p_x'|N_X,T)}
%\end{equation}
%Here, $P(N_{``X"}|p_x',N_X,T)$ is a binomial distribution, although this is not
%true strictly speaking given that our agents are learning and the values of the
%confusion matrix are moving. Modelling the learning curve of our users is yet
%another complicated extension we could think about.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%  REFERENCES
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% MNRAS does not use bibtex, input .bbl file instead. 
% Generate this in the makefile using bubble script in scriptutils:

% bubble -f paper-lcr.tex references.bib 
% \input{paper-lcr.bbl}

\bibliographystyle{apj}
\bibliography{references}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\label{lastpage}
\bsp

\end{document}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
