%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% Space Warps I: Experiment Design
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\documentclass[useAMS,usenatbib,a4paper]{mn2e}
%% letterpaper
%% a4paper

\voffset=-0.6in

% Packages:
\input psfig.sty
\usepackage{xspace}
\usepackage{graphicx}
\usepackage{amssymb}
\usepackage{amsmath}

% Macros:
\input{macros.tex}
\input{addresses.tex}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\title[\sw I]
{\SW: I. Crowd-sourcing the Discovery of Gravitational Lenses}
    
\author[Marshall et al.]{%
  \input{sw-system-authors.tex}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}
             
\date{to be submitted to MNRAS}
\pagerange{\pageref{firstpage}--\pageref{lastpage}}\pubyear{2013}

\maketitle           

\label{firstpage}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{abstract} 

\sw is a web-based service
 that enables the discovery of strong
gravitational lenses in wide-field imaging surveys by arbitrarily large
numbers of people. Carefully produced color composite images are
displayed to the classifiers via a flexible interface, which records
their estimates of the positions of candidate lensed features. Simulated
lenses, and expert-classified non-lenses, are inserted into the stream
at random intervals; this training set is used to give the classifiers
feedback on their performance as well as estimate a dynamically-updated
probability for any given image containing a lens. High probability
systems are filtered into the Zooniverse \Talk discussion system; this
smaller sample is further classified and analysed by volunteers into a
final set. We analyze the classification of the training set itself, and
find that 1) $<$volunteers learn$>$... 2) $<$the filter works$>$... 

\end{abstract}

% Full list of options at http://www.journals.uchicago.edu/ApJ/instruct.key.html

\begin{keywords}
  gravitational lensing   --
  methods: statistical    
\end{keywords}

\setcounter{footnote}{1}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Introduction}
\label{sec:intro}

Scientific motivation. Applications of lenses: group-scale arcs,
galaxy-galaxy lenses, lensed quasars. 

Problem of rarity. Imaging surveys. Problem of purity/false positives.

Review of progress to date. Methods in SL2S, SQLS. Contrast with SLACS. 

Scaling to wide field era. Automated methods: problems. Need for good
training sets. Need for quality control: always present.

Novel solution: crowd-sourcing. Brief review of similar problems.
PlanetHunters. \sw as an experiment.

In this paper, we describe the \sw website, an online system that
enables crowd-sourced detection of gravitational lenses. 
Other papers in this series will present new gravitational lenses
discovered in our first imaging survey dataset, and investigate the
differences between lens detections made in \sw and those made with
automated techniques. Here, we try to answer the following questons:
\begin{itemize}

\item How reliably can we find gravitational lenses using the \sw
system? What is the completeness of the sample produced?

\item How noisy is the system? What is the purity of the sample
produced?

\item How accurately can gravitational lenses be located on the sky
during the identification process?

\item How quickly can lenses be detected, and non-lenses be rejected?
How many classifiers are needed per target, and how quickly can they
mobilise?

\end{itemize}

In \Sref{sec:design} we introduce the \sw system, describing and
explaining its various features. We then briefly 



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Experiment Design}
\label{sec:design}

Unfamiliar objects: need to learn what lenses look like, fast. Rare
objects: need to be able to reject rapidly, and get through sample.
Confusion with non-lenses: further filtering via scientific discussion
and further classification (voting). 

% % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % 

\subsection{Tutorial Material}
\label{sec:design:tutorial}

Learning what lenses do: Spotter's Guide and LensToy. Learning how
lenses work (science page, FAQ). 

Inline tutorial. Merge into stream. Instant feedback, positive and
negative. Anecdotal support for this.


% % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % 

\subsection{The Classification Interface}
\label{sec:design:classification}

Image presentation. Consistency in size and appearance. 

Interface fast due to pre-loading of images, and minimizing interaction.

Spotting lenses: Markers to be placed. Two reasons: first, to give good
feedback. Second, to improve quality of discussion later.

Quick dashboard provides simple ways to explore further: zoom, contrast
controls.

Non-lenses marked? Favourite button instead, enabling serendipitous
discovery of other interesting things, separate from lenses.


% % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % 

\subsection{Collaborative Filtering with ``Talk''}
\label{sec:design:classification}

Feeding in collections, following online analysis. 
Taking subjects to Talk individually, too.

Mental modeling. Further inspection on Dashboard. Voting. 

Final sample generation.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Data}
\label{sec:data}

Training data and test data. Refer to Paper II for CFHTLS details.

Presentation of images. Arcsinh stretch, calibration. Approximately
optimized.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Identification Analysis}
\label{sec:IDanalysis}

In this section we outline our methodology for interpreting the
interactions of the volunteers with the identification interface.  Each
classification made is logged in a database, storing subject IDs,
(anonymous) volunteer IDs, a timestamp and the classification results. 
The {\it kind} of subject -- whether it is a training subject (a 
simulated lens or a known non-lens) or a test subject (an unseen image
drawn from the survey) -- is also recorded. For all subjects, the
positions of all Markers are recorded, in pixel coordinates. For
training subjects, we also store the ``classification'' of the subject
as a lens, or a non-lens, and also the type of object present in the
image. These types are summarized in \Tref{tab:objecttypes}. 
This classification is used to provide instant feedback, but is also the
basic measurement used in a probabilistic classification of every
subject based on all image views to date.

We perform a ``Pseudo-online'' analysis of the classifications, 
updating a probabilistic model of every (anonymous) volunteer's
expertise, and every subject (in both the training and test sets), on a
daily basis. This allows us to track the speed with which the crowd
learns about lenses, and also gives us a dynamic estimate of the
posterior probability for  any given  subject being a lens, given all
classifications of it. Assigning thresholds in this lens probability
allows us to make good decisions about whether or not to accept a
subject into the collection of candidates visible in \Talk, and also
whether or not to  carry on classifying a subject at all. 

In this paper we focus on the training data,  investigating  how the 
crowd's ability to identify gravitational lenses during the course of
the project, and the completeness and purity of the lens candidate
sample generated.

Where to describe probabilistic classifier stuff? Here or in an
appendix? Not sure.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Identification Results}
\label{sec:IDresults}

Understanding crowd, so we can help them learn faster. Understanding
images given the crowd, so we can find lenses.

% % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % 

\subsection{Learned Behavior}
\label{sec:results:learning}

% % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % 

\subsection{Sample completeness and purity}
\label{sec:results:learning}

Vary the acceptance and rejection thresholds, look at completeness and
purity as a function of this. How much time can be saved by retiring
subjects into candidate list or rejection list?


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Discussion}
\label{sec:discuss}

Challenges for future.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Conclusions}
\label{sec:conclude}

We draw the following conclusions:

\begin{itemize} 

\item Ability to detect lenses

\item What we can say about the next steps

\end{itemize}

Summarize, end.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%  ACKNOWLEDGMENTS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section*{Acknowledgements}
 
\input{acknowledgments.tex}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%  APPENDICES
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\appendix

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Probabilistic Classification Model}
\label{appendix:probmodel}

Our aim is to enable the construction of a sample of good lens
candidates. Since we aspire to making logical  decisions, we define a 
``good candidate'' as one which has a high posterior probability of
being a lens, given the data: $\pr(\LENS|\data)$. Our problem is to
approximate this probability. The data in our case are the pixel values
of a colour image. However, we can greatly compress these complex, noisy
sets of data by asking each classifier what they think about them. A
complete  classification in \sw consists of a set of Marker positions,
or none at all. The null set represents the statement from the
classifier that the image in question is $\saidNOT$ a lens, while the
placement of any  Markers indicates that the classifier considers this
image to contain a $\saidLENS$. 

We simplify the problem by only using the Marker positions to assess
whether the classifier  correctly assigned the classification
$\saidLENS$ or $\saidNOT$ after viewing (blindly) a member of the
training set of subjects. We model each classifier's expertise with a
simple 2 by 2 ``confusion matrix,'' which is updated every time that
volunteer classifies a training subject using the following 
very simple estimate:
\be
  \pr(``X"|X,T) \approx \frac{N_{``X"}}{N_X}.
  \label{eq:app:fraction}
\ee
Here, $X$ stands for the true classification of the subject, \ie either
$\LENS$ or $\NOT$, while $``X''$ is the corresponding classification
made by the classifier on viewing the subject. $N_X$ is the number of
lenses the classifier has been shown, while $N_{``X"}$ is the number of 
times the classifier got their classifications right. $T$ stands for all
$N_{\LENS} + N_{\NOT}$ training data that the classifier has seen to
date.

The full confusion matrix of the $k^{\rm th}$ classifier is therefore:
\be
  \mathcal{M}^k = 
  \begin{bmatrix}
    \pr(\saidLENS|\NOT,T_k) & \pr(\saidLENS|\LENS,T_k) \\
    \pr(\saidNOT |\NOT,T_k) & \pr(\saidNOT |\LENS,T_k)
  \end{bmatrix}.
\ee
Note that these probabilities are normalized, \eg 
$\pr(\saidNOT |\NOT) = 1 - \pr(\saidLENS|\NOT)$.

Now, when this classifer views a test subject, 
it is this confusion matrix that will tell us how to update the
probability of that test subject being a $\LENS$. Let us suppose that
this subject has never been seen before: the prior probability that it
is (or contains) a lens is 
\be
  \pr(\LENS) = p_0
\ee
where we have to assign a value for $p_0$. In the CFHTLS, we might expect
something like 100 lenses in 430,000 images, so $p_0 = 2\times10^{-4}$
is a reasonable estimate. The classifier makes a classification $C_k$ 
($= \saidLENS or \saidNOT$).
We can apply Bayes' Theorem to update this 
prior probability into a posterior one using this new information:
\begin{align}
  & \pr(\LENS|C_k,T_k) = \\ \notag
  & \frac{\pr(C_k|\LENS,T_k)\cdot\pr(\LENS)}
{\left[ \pr(C_k|\LENS,T_k)\cdot\pr(\LENS) + \pr(C_k|\NOT,T_k)\cdot\pr(\NOT) \right]},
  \label{eq:app:first}
\end{align}
which can be evaluated numerically using the elements of the confusion
matrix. 

As an example, suppose we have a {\it perfect} classifier, with
confusion matrix
\be
  \mathcal{M}^{\rm perfect} = 
  \begin{bmatrix}
    0.0 & 1.0 \\
    1.0 & 0.0
  \end{bmatrix},
\ee
who is given a fresh subject that actually is a $\LENS$. 
They submit $C = \saidLENS$. 
The resulting posterior probability for the classified
subject being a $LENS$ would be
\begin{align}
  \pr(\LENS|\saidLENS,T_k) &= \frac{1.0 \cdot p_0}
           {\left[ 1.0\cdot p_0 + 0.0\cdot(1 - p_0) \right]}
   &= 1.0
\end{align}
as expected. Meanwhile, an {\it obtuse} classifier, who always gets it
wrong and has the column-swapped confusion matrix, 
\be
  \mathcal{M}^{\rm obtuse} = 
  \begin{bmatrix}
    1.0 & 0.0 \\
    0.0 & 1.0
  \end{bmatrix},
\ee
would submit $C = \saidNOT$ for this subject, also leading to
\begin{align}
  \pr(\LENS|\saidNOT,T_k) &= \frac{1.0 \cdot p_0}
           {\left[ 1.0\cdot p_0 + 0.0\cdot(1 - p_0) \right]}
   &= 1.0.
\end{align}
This shows that  classifiers who are systematically wrong can 
nevertheless provide useful information. Indeed, the information content
of each classification can be estimated before the next classification
is made, and computed just from the confusion matrix. We define the 
expected information per classification, or expertise, in bits, as
\be
  \langle I_k \rangle = \frac{1}{2} 
                        \sum_{i} \left( \mathcal{M}^k_{i}  
              + \mathcal{M}^k_{i}\cdot\log_2{\mathcal{M}^k_i} \right),
  \label{eq:app:info}
\ee
where $i$ runs over all 4 elements of the confusion matrix 
$\mathcal{M}^k_i$. The second term in the sum is recognisable as a
negative entropy, while the first term and the factor of 1/2
account for the way the confusion matrix encodes the results for the
classification of two types of subject, and that the probabilities
associated with each type are normalised. \Eref{eq:app:info} gives the
required result, that 
both the {\it perfect} and
{\it obtuse} classifiers contribute 1 bit of information each, per
classification. 
The only classifier that contributes the minimum
information per classification, zero bits, is the {\it random} 
classifier,
whose confusion matrix elements are all 0.5.   We conservatively 
initialise all classifiers' confusion matrices to that of  the random
classifier, effectively assuming that all users are equally poorly
informed about what a $\LENS$ is, and what is $\NOT$. Plotting 
$\langle I_k \rangle$ as a function of time will illustrate the learning
process undergone by the $k^{\rm th}$ classifier.

Suppose the $k+1^{\rm th}$ classifier now submits a classification,
on this subject. We can generalise \Eref{eq:app:first} by replacing the
prior probability with the current posterior probability:
\begin{align}
  \pr(\LENS & |C_{k+1},T_{k+1},\data) = \\ \notag
  & \frac{1}{Z} \pr(C_{k+1}|\LENS,T_{k+1}) \cdot \pr(\LENS|\data) \\ \notag
{\rm where}\;\; Z = & \pr(C_{k+1}|\LENS,T_{k+1})\cdot\pr(\LENS|\data) \\ \notag
      & + \pr(C_{k+1}|\NOT,T_{k+1})\cdot\pr(\NOT|\data),
  \label{eq:app:update}
\end{align}
and $\data = \{C_k,T_k\}$ is the set of all previous
classifications, each given their classifier's training subjects.
$\pr(\LENS|\data)$ is the fundamental property of each test subject that
we are seeking. We track $\pr(\LENS|\data)$ as a function of time,
and by comparing it to a lower or upper thresholds, make decisions about
whether to retire the subject from the classification interface or
promote it in \Talk, respectively.





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%  REFERENCES
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% MNRAS does not use bibtex, input .bbl file instead. 
% Generate this in the makefile using bubble script in scriptutils:

% bubble -f paper-lcr.tex references.bib 
% \input{paper-lcr.bbl}

\bibliographystyle{apj}
\bibliography{references}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\label{lastpage}
\bsp

\end{document}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
